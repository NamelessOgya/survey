{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc9ec49-b908-4541-8fa8-5e76c95b117f",
   "metadata": {},
   "source": [
    "# Mnistテーマにパイプラインを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3c0bcf-f978-4be2-b246-0c0300020edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestamp generate\n",
    "from datetime import datetime\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "#GCP\n",
    "from google.cloud import storage as gcs\n",
    "import google.cloud.aiplatform as aip\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "# kubeflow SDK\n",
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdd750fe-bae9-410f-967d-0c3b866d3f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"project_id\": \"test-hyron\",\n",
    "    \"bucket_name\": \"aruha-mnist\",\n",
    "    \"pipeline_root\": \"gs://aruha-mnist/pipeline_log\", \n",
    "    \n",
    "    \"model_url\": \"gs://aruha-mnist/mlpos/mlpos/model\",\n",
    "    \"code_url\": \"gs://aruha-mnist/mlpos/mlpos/code\",\n",
    "    \n",
    "    \"train_url\":\"gs://aruha-mnist/mlpos/mlpos/data/mnist_train.csv\",\n",
    "    \"test_url\" :\"gs://aruha-mnist/mlpos/mlpos/data/mnist_test.csv\",\n",
    "    \"valid_proportion\": 0.2,\n",
    "    \"TIMESTAMP\": TIMESTAMP,\n",
    "    \n",
    "    \"epochs\" : 5,\n",
    "    \"batch_size\" : 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f45e2cd2-0ad9-4a8c-a447-5b8b4d3b1c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = params[\"project_id\"]\n",
    "client = gcs.Client(project_id)\n",
    "\n",
    "bucket_name = params[\"bucket_name\"]\n",
    "bucket = client.get_bucket(bucket_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1453c6cc-883b-472d-a457-e23c2d301c5c",
   "metadata": {},
   "source": [
    "# コンポーネントの定義  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abd5fda1-b62e-462b-bd0d-ec8c59f494fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@component(base_image='tensorflow/tensorflow:latest')\n",
    "def preprocess_conduct(\n",
    "    project_id: str,\n",
    "    bucket_name: str,\n",
    "    train_url:str,\n",
    "    test_url :str,\n",
    "    valid_proportion: float,\n",
    "    \n",
    "    train_dataset: Output[Dataset],\n",
    "    test_dataset: Output[Dataset],\n",
    "    valid_dataset: Output[Dataset]\n",
    "):\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from google.cloud import storage as gcs\n",
    "    from io import BytesIO\n",
    "    \n",
    "    \n",
    "    project_id = project_id\n",
    "    client = gcs.Client(project_id)\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    \n",
    "    def data_load_from_url(bucket, bucket_name, url):\n",
    "        path_ = url.split(\"gs://\" + bucket_name + \"/\")[1]\n",
    "        blob = bucket.blob(path_)\n",
    "        content = blob.download_as_bytes()\n",
    "        output = np.loadtxt(BytesIO(content), delimiter=',')\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    train = data_load_from_url(bucket, bucket_name, train_url)\n",
    "    test  = data_load_from_url(bucket, bucket_name, test_url)\n",
    "    \n",
    "    np.random.shuffle(train)\n",
    "    np.random.shuffle(test)\n",
    "    \n",
    "    #時間短縮のため\n",
    "    train = train[:10000]\n",
    "    test = test[:1000]\n",
    "\n",
    "    valid_data  = train[:int(len(train) * (valid_proportion))]\n",
    "    train_data  = train[int(len(train) * (valid_proportion)) + 1:]\n",
    "    test_data   = test\n",
    "    \n",
    "    #ファイルの受け渡しはsave / loadによってなされる。kubeflowは保存場所を提供しているにすぎない。\n",
    "    _train_dataset = tf.data.Dataset.from_tensor_slices(( train_data[:, 1:]/255.0, train_data[:, 0].reshape(-1, 1)))\n",
    "    _test_dataset = tf.data.Dataset.from_tensor_slices(( test_data[:, 1:]/255.0, test_data[:, 0].reshape(-1, 1)))\n",
    "    _valid_dataset = tf.data.Dataset.from_tensor_slices(( valid_data[:, 1:]/255.0, valid_data[:, 0].reshape(-1, 1)))\n",
    "    \n",
    "    \n",
    "    _train_dataset.save(train_dataset.path, compression=None, shard_func=None, checkpoint_args=None)\n",
    "    _test_dataset.save(test_dataset.path, compression=None, shard_func=None, checkpoint_args=None)\n",
    "    _valid_dataset.save(valid_dataset.path, compression=None, shard_func=None, checkpoint_args=None)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24d15654-e126-4ed2-93f3-d969da9ea612",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image='tensorflow/tensorflow:latest')\n",
    "def train_model_conduct(\n",
    "    train_dataset :Input[Dataset],\n",
    "    valid_dataset :Input[Dataset],\n",
    "    \n",
    "    batch_size    :int,\n",
    "    epochs        :int,\n",
    "    \n",
    "    model         :Output[Artifact]\n",
    "\n",
    "):\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "    import shutil\n",
    "    import csv\n",
    "    \n",
    "    class Simple_CNN(tf.keras.Model):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv_2d_0 = tf.keras.layers.Conv2D(32, (4,4), activation = \"relu\", input_shape = (28,28,1))\n",
    "            self.pooling_0 = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "\n",
    "            self.conv_2d_1 = tf.keras.layers.Conv2D(32, (4,4), activation = \"relu\")\n",
    "            self.pooling_1 = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "\n",
    "            self.conv_2d_2 = tf.keras.layers.Conv2D(32, (4,4), activation = \"relu\")\n",
    "\n",
    "            self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "            self.dense_0 = tf.keras.layers.Dense(64, activation = \"relu\")\n",
    "            self.dense_1 = tf.keras.layers.Dense(10, activation = \"softmax\")\n",
    "\n",
    "\n",
    "        def call(self, inputs):\n",
    "            x = tf.reshape(inputs, [-1,28,28,1])\n",
    "            x = self.pooling_0(self.conv_2d_0(x))\n",
    "            x = self.pooling_1(self.conv_2d_1(x))\n",
    "            x = self.conv_2d_2(x)\n",
    "\n",
    "            x = self.flatten(x)\n",
    "            x = self.dense_0(x)\n",
    "            x = self.dense_1(x)\n",
    "\n",
    "            return x\n",
    "        \n",
    "    def train_model(batch_size,epochs, train_dataset, valid_dataset):\n",
    "        model = Simple_CNN()\n",
    "        model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs = epochs, \n",
    "            validation_data = valid_dataset\n",
    "        )\n",
    "\n",
    "        return model, history\n",
    "    \n",
    "    _train_dataset = tf.data.Dataset.load(train_dataset.path)\n",
    "    _valid_dataset = tf.data.Dataset.load(valid_dataset.path)\n",
    "    \n",
    "    _model, _history = train_model(batch_size, epochs,_train_dataset, _valid_dataset)\n",
    "    \n",
    "    _model.save(model.path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e715afb-4aae-4dd5-aae0-be23222f50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image='tensorflow/tensorflow:latest')\n",
    "def test_model_conduct(\n",
    "    test_dataset: Input[Dataset], \n",
    "    model       : Input[Artifact],\n",
    "    \n",
    "    loss        : Output[Artifact],\n",
    "    accuracy    : Output[Artifact]\n",
    "):\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    _test_dataset = tf.data.Dataset.load(test_dataset.path)\n",
    "    \n",
    "    _model = tf.keras.models.load_model(model.path)\n",
    "        \n",
    "    _loss, _accuracy = _model.evaluate(_test_dataset)\n",
    "    \n",
    "    with open(loss.path, \"w\") as f:\n",
    "        f.write(str(_loss))\n",
    "        \n",
    "    with open(accuracy.path, \"w\") as f:\n",
    "        f.write(str(_accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1cd791-77f7-43d6-b05c-65900ee875e6",
   "metadata": {},
   "source": [
    "# パイプラインの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0be75141-48d1-4f6a-9b82-3e2f5198b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root=params[\"pipeline_root\"],\n",
    "    name=\"example-pipeline\",\n",
    ") \n",
    "def pipeline(\n",
    "    project_id: str,\n",
    "    bucket_name: str,\n",
    "    pipeline_root: str, \n",
    "    \n",
    "    model_url: str,\n",
    "    code_url: str,\n",
    "    \n",
    "    train_url:str,\n",
    "    test_url :str,\n",
    "    \n",
    "    valid_proportion: float,\n",
    "    TIMESTAMP: str,\n",
    "    \n",
    "    batch_size: int,\n",
    "    epochs : int\n",
    "):\n",
    "    # データの前処理など\n",
    "    preprocess_task = preprocess_conduct(\n",
    "        project_id, \n",
    "        bucket_name, \n",
    "        train_url,\n",
    "        test_url,\n",
    "        valid_proportion\n",
    "    )\n",
    "    \n",
    "    # \n",
    "    train_model_task = train_model_conduct(\n",
    "        train_dataset = preprocess_task.outputs[\"train_dataset\"],\n",
    "        valid_dataset = preprocess_task.outputs[\"valid_dataset\"],\n",
    "\n",
    "        batch_size    = batch_size,\n",
    "        epochs        = epochs\n",
    "    )\n",
    "\n",
    "    test_model_task = test_model_conduct(\n",
    "        test_dataset = preprocess_task.outputs[\"test_dataset\"], \n",
    "        model        = train_model_task.outputs[\"model\"]\n",
    "    )\n",
    "    \n",
    "\n",
    "#     save_model_task = save_model_conduct(\n",
    "#         model_dir  = model_dir,\n",
    "#         model_name = model_name,\n",
    "#         base_dir   = base_dir,\n",
    "#         code_dir   = code_dir,\n",
    "\n",
    "#         model      = train_model_task.outputs[\"model\"],\n",
    "#         TIMESTAMP  = TIMESTAMP,\n",
    "#         loss       = test_model_task.outputs[\"loss\"],\n",
    "#         accuracy   = test_model_task.outputs[\"Accuracy\"]\n",
    "#     )\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3152ab32-76a9-45d9-a747-b0ca48b77338",
   "metadata": {},
   "source": [
    "# コンパイル/実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be693308-8c5b-4c1c-a441-bc42c4a3e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler  # noqa: F811\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"test_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98c92304-29d6-499b-811f-6d5ffe9ebd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/784694072347/locations/us-central1/pipelineJobs/example-pipeline-20221120130857\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/784694072347/locations/us-central1/pipelineJobs/example-pipeline-20221120130857')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/example-pipeline-20221120130857?project=784694072347\n",
      "PipelineJob projects/784694072347/locations/us-central1/pipelineJobs/example-pipeline-20221120130857 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/784694072347/locations/us-central1/pipelineJobs/example-pipeline-20221120130857 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/784694072347/locations/us-central1/pipelineJobs/example-pipeline-20221120130857 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/784694072347/locations/us-central1/pipelineJobs/example-pipeline-20221120130857\n"
     ]
    }
   ],
   "source": [
    "DISPLAY_NAME = \"test\" + TIMESTAMP\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"test_pipeline.json\",\n",
    "    pipeline_root=params[\"pipeline_root\"],\n",
    "    parameter_values= params,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e537affa-9e08-4545-9d89-daad1b293b87",
   "metadata": {},
   "source": [
    "# 今後対応したいこと"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c042513-4b03-430f-9f1a-00ab1da75146",
   "metadata": {},
   "source": [
    "- ディレクトリの構成などをもうすこしちゃんと定義\n",
    "- 結果をcsv出力してIBツールで確認できるように\n",
    "- 学習/検証など主要部分だけ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d891ba1-5fa1-4c53-8c83-a194c8a0638e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
