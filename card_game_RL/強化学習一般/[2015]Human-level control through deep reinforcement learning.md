# RECURRENT EXPERIENCE REPLAY IN DISTRIBUTED REINFORCEMENT LEARNING
url:https://training.incf.org/sites/default/files/2023-05/Human-level%20control%20through%20deep%20reinforcement%20learning.pdf

# summary  
## どんなもの  
- 画面入力のみから学習可能なアーキテクチャDQNを開発し、43/49のゲームで既存のより詳細な情報をもとにしたモデルよりも高いパフォーマンスを発揮した。

## 先行技術と比べてどこがすごい。
- 強化学習エージェントはこれまで有用な特徴量を作り出せるドメインや、完全に観測可能な低次元の状態空間を持つものに限られてきたなかで、画面情報というごく限られた情報から学習を可能にしたこと。
- これまで強化学習環境での深層学習はむずかしかったが、経験再生/周期的なパラメータ更新によってこれを可能にした。

## 技術や手法の特異点はどこか  
- 画像学習にはCNNを利用
- 経験再生
    - データを学習させる際に、(s,a,t,s')を保存しておき、そこからランダムサンプリングしたデータで学習を行う。
        - これにより、データの使用効率が向上する。
        - サンプル間の相関を減らすことができる。
        - オンポリシー問題の緩和
            - オンポリシー学習では現在のポリシーが次に生成されるデータを決定してしまう。ランダム化によってこれを緩和できる。
- パラメータ更新の工夫
    - モデル更新を周期的に行うことで、観測値とモデルの相関を減らした。
    - Q価推定用のモデルを二つ用いる。
        - 実際の行動の際に使うモデル(target model)とパラメータ更新対象のモデルを分ける。
        - パラメータ更新を行うモデルのパラメータを、C回更新するごとにtarget modelにコピーする。
## 議論はあるか  


## 本文  
- 強化学習はニューラルネットワークを用いて学習すると不安定になったり発散になったりすることがある。
    - Qに対して小さなアップデートをすると、ポリシーが著しく変わることがあり、それがデータ分布とaction_value(Q)とtarget valuesの間の相関に大きな影響を与える。
    - これに対処するために2点変更点を加えたQ-learningを提案する。
        - 経験再生
        - Q値のアップデートを周期的にすることで、targetとの相関を減らす。
    - neural fitted Q-iterationはstable methodの例としてあげられるが、これらは数百のイテレーションを経てネットワークをトレーニングするもので、本論文と比べて非効率である。
- 比較実験  
    - 方法
        - Atariの49のゲームに対してテストを行った。
            - 実験には同一のネットワークを用いた。
    - 結果
        - 強化学習シグナルから安定した学習が行えた。
        - 43/49のゲームにおいて、事前知識なしで既存モデルを上回るパフォーマンスを発揮した。
        - 半分以上のゲームにおいて、人間スコアの75パーセント以上を達成した。
- tSNEでの表現学習の解析  
    - 似た状況は似たような出力になっていた。  
- discussion
    - DQNは横スクロールやシュータなど様々なゲームをプレイできた。
    - 長期的依存性を学習しているケースも確認できた。
# METHODS  
- ネットワークに関して
    - これまでのネットワークはアクションと常態を入力としていた。
        - これにより、アクション数に比例する大きなコストがかかっていた
    - DQNではステートのみを入力にし、アクション数と同じだけの出力をするモデルを用いた。
    これにより、全アクションのQ値を一度に計算できるように。
- 学習に関して
    - possibleなリワードを1に、negativeなリワードを-1に置き換えた。
    - batchサイズ32のRMSPropを用いた
    - frame skipping
        - すべてのフレームごとではなく、kフレームごとにアクションの選択を行う。
        - 選択していない間は直前のアクションがリピートされる。
        - これにより学習時間とアクションリソースを軽減

