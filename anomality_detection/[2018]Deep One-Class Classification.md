# [2018]Deep One-Class Classification  
One class SVDDをDeep learningで行うというもの。

## one class学習の問題設定  
2種類の分類タスクにおいて、片方のクラスのデータしか得られない場合、もしくは片方のデータが極端に少ない場合に用いる学習方法  
異常検知などで用いられる。  

## 基本アイディア  
![image](https://user-images.githubusercontent.com/54636129/200241655-a61494d5-3cda-427b-a97d-4ca26aea5bc8.png)  
データに対してニューラルネットワークを作用させることによって、データを別空間にmapする。  
その際に、データができるだけ円中心cに集まるように学習する。  
このとき、異常なデータは円の外に存在することが期待され、これにより学習に使用したクラスではないクラスを学習することができる。  
  
## 学習方法  
### 誤差関数  
- 学習データに片方のクラスしか含まれていないとき(One-class)  
  超珠の中心をcとして、以下のような誤差関数が設定される。  
  超級からの距離ができるだけ小さくなるように学習する。  
  第二項目はweight decay    
![image](https://user-images.githubusercontent.com/54636129/200242550-c5dd4919-567a-4108-bcd4-3b0cf0c15792.png)  
- 学習データにもう片方のクラスがわずかに含まれる場合(Soft-bound)  
以下のように、超珠半径Rをパラメータとして考え、これを最小化するように学習する。  
わずかに含まれる他クラスデータを許容できるようにRを超えたものには罰則を与えるような仕組みになっている。  
（こっちは興味ないのでとばし）   
![image](https://user-images.githubusercontent.com/54636129/200242389-692114cb-2758-48f0-8506-d5fbf5d33eec.png)  

### 学習の際の注意  
- 円中心に0を選ばない  
  すべての点を0にmapするようなNNが出来上がってしまうため。  
- ニューラルネットにbias項を加えない  
  同じ上。  
  bias項部分で超球中心分だけ並行移動することで↑と同じ状況が実現  
- 活性化関数にReLUを用いる  
  soft-maxなど、上下にboundaryがある関数を使うとすべての点をcにmapするNNが実現できてしまう。

## 結果  

